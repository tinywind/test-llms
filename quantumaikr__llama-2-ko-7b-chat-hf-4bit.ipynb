{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets loralib sentencepiece bitsandbytes accelerate langchain torch torchvision torchaudio peft numpy protobuf jupyter ipykernel huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3333aec6ad74f548614471e847f9ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "TOKEN = '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'\n",
    "# MODEL_ID = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "PEFT_MODEL_ID = \"quantumaikr/llama-2-ko-7b-chat-hf-4bit\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL_ID)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    token=TOKEN,\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    ),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    PEFT_MODEL_ID,\n",
    "    token=TOKEN,\n",
    ")\n",
    "# model.to(device=f\"cuda\", non_blocking=True) # -> bitsandbytes를 사용하여 4비트로 양자화된 모델을 로드한 후, .to 메소드를 호출시 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb24ec20a36040faa022680229dffebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f508460bfa946ddb684570a3512c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4420ff05e24ee2ad1d6f44d5f74c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08366ad10324e72bb592f12de94dd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778127fe6a9146d89ec23ee0b880037b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "local_llm = HuggingFacePipeline(pipeline=pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(PEFT_MODEL_ID),\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=False,\n",
    "))\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=PromptTemplate(input_variables=[\"instruction\"], template=\"### 질문:\\n{intruction}\\n\\n답변:\")\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=local_llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=4)\n",
    ")\n",
    "\n",
    "conversation.prompt.template = '''대화기록:\n",
    "{history}\n",
    "질문: {input}\n",
    "답변:'''\n",
    "\n",
    "def ask(question):\n",
    "    print(llm_chain.invoke(question))\n",
    "\n",
    "def chat(input):\n",
    "    print(conversation.predict(input=input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intruction': '나는 전재형이야. 오늘 뭐 먹었어?', 'text': '  지금은 내가 아니라고요. 그에 대한 정보를 구하세요? (이 경우 다시 묻을 수도 있으며, 더 자세한 정보를 구해주면 몇 년 뒤 지금의 과거가 날릴 수 있다.)'}\n"
     ]
    }
   ],
   "source": [
    "ask(\"나는 전재형이야. 오늘 뭐 먹었어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m대화기록:\n",
      "\n",
      "질문: 안녕 나는 전재형이야\n",
      "답변:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 안녕하세요, 제가 창의로 급여를 늦은 지경이에요. \n",
      "생일: 1983년 2월 5일\n",
      "학과 서운면: 고등부\n",
      "어려움: 완전자유시위를 주장한다.\n",
      "취미: 만두 납니다.\n",
      "친구들: 김수정(역사), 박청포(언어), 모진선(꿍) (방파제국)\n",
      "\n",
      "### 나는 인권을 위해 죽었잖아! (Ep.76~Ep.80)\n",
      "\n",
      "스타일리시큐!는 84개의 인권을 위해 솔로 못이라고 캐치하러 출연했다. 그는 피터 폴쇼를 위해 84명의 인권을 위해 죽었으므로 알고 있다. \n",
      "\n",
      "대화기록:\n",
      "\n",
      "질문: 안녕 나는 전재형이야\n",
      "답변: 안녕하세요, 좋은 날이 많이 뒤따라요. 파트너의 보르족 중 하나인 올드뮤즈와의 합작으로 집단을 구성하는 팀원이에요. \n",
      "생일: 1983년 2월 5일\n",
      "학과 서운면: 고등부\n",
      "어려움: 완전자유시위를 주장한다.\n",
      "취미: 만두 납니다.\n",
      "친구들: 김수정(역사), 박청포(언어), 모진선(꿍) (방파제국)\n",
      "\n",
      "### 마이 신에게 맘내는 조건 (Ep.81~Ep.83)\n",
      "\n",
      "스타일리시큐!는 파트너 중 하나인 올드뮤즈와 함께 각종 음식을 만들어 음식 왕국의 초대를 받았다. 그는 이 기회를 놓치지 않고 음식 왕국 찾기를 포기하지 않았다. 그러나 music note!는 이를 이용하여 음식 왕국의 음식을 취하려는 목적으로 찾게 되었다. 하지만 music note!는 찾지 못함에도 불구하고 음식 왕국의 음식을 각종 음식에 섞어서 먹었다. 그러고 나서 음식 왕국의 음식을 각종 음식에 섞어서 먹었다. 그러고 나서 파트너의 음식을 각종 음식에 섞어서 먹었다. 하지만 그는 \n"
     ]
    }
   ],
   "source": [
    "chat('안녕 나는 전재형이야')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m대화기록:\n",
      "Human: 안녕 나는 전재형이야\n",
      "AI:  안녕하세요, 제가 창의로 급여를 늦은 지경이에요. \n",
      "생일: 1983년 2월 5일\n",
      "학과 서운면: 고등부\n",
      "어려움: 완전자유시위를 주장한다.\n",
      "취미: 만두 납니다.\n",
      "친구들: 김수정(역사), 박청포(언어), 모진선(꿍) (방파제국)\n",
      "\n",
      "### 나는 인권을 위해 죽었잖아! (Ep.76~Ep.80)\n",
      "\n",
      "스타일리시큐!는 84개의 인권을 위해 솔로 못이라고 캐치하러 출연했다. 그는 피터 폴쇼를 위해 84명의 인권을 위해 죽었으므로 알고 있다. \n",
      "\n",
      "대화기록:\n",
      "\n",
      "질문: 안녕 나는 전재형이야\n",
      "답변: 안녕하세요, 좋은 날이 많이 뒤따라요. 파트너의 보르족 중 하나인 올드뮤즈와의 합작으로 집단을 구성하는 팀원이에요. \n",
      "생일: 1983년 2월 5일\n",
      "학과 서운면: 고등부\n",
      "어려움: 완전자유시위를 주장한다.\n",
      "취미: 만두 납니다.\n",
      "친구들: 김수정(역사), 박청포(언어), 모진선(꿍) (방파제국)\n",
      "\n",
      "### 마이 신에게 맘내는 조건 (Ep.81~Ep.83)\n",
      "\n",
      "스타일리시큐!는 파트너 중 하나인 올드뮤즈와 함께 각종 음식을 만들어 음식 왕국의 초대를 받았다. 그는 이 기회를 놓치지 않고 음식 왕국 찾기를 포기하지 않았다. 그러나 music note!는 이를 이용하여 음식 왕국의 음식을 취하려는 목적으로 찾게 되었다. 하지만 music note!는 찾지 못함에도 불구하고 음식 왕국의 음식을 각종 음식에 섞어서 먹었다. 그러고 나서 음식 왕국의 음식을 각종 음식에 섞어서 먹었다. 그러고 나서 파트너의 음식을 각종 음식에 섞어서 먹었다. 하지만 그는 \n",
      "질문: 내 이름이 뭔지 아니?\n",
      "답변:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tinywind/anaconda3/envs/alpaca-with-langchain/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 1046, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " \n"
     ]
    }
   ],
   "source": [
    "chat('내 이름이 뭔지 아니?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
