{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets loralib sentencepiece bitsandbytes accelerate langchain torch torchvision torchaudio peft numpy protobuf jupyter ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc58fe0491c4be6a29701c7e54e1962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "import torch\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model_id = 'squarelike/llama-2-koen-story-13b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "model = quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "def gen(string):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        gened = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        return tokenizer.decode(gened[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Is Cat cute?\n",
      "I'm not sure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gen('Is Cat cute?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GenerationConfig, TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# streamer = TextStreamer(tokenizer)\n",
    "def gen(input):\n",
    "    with torch.no_grad():\n",
    "        gened = model.generate(\n",
    "            **tokenizer(f\"### instruction: {input}\\n\\n### Response: \", return_tensors=\"pt\").to(device),\n",
    "            generation_config = GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                top_p=0.2,\n",
    "                top_k=1000,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=2,\n",
    "                early_stopping=True,\n",
    "                do_sample=True,\n",
    "            ),\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # streamer=streamer,\n",
    "        )\n",
    "        result_str = tokenizer.decode(gened[0])\n",
    "        start_tag = f\"\\n\\n### Response: \"\n",
    "        start_index = result_str.find(start_tag)\n",
    "\n",
    "        if start_index != -1:\n",
    "            result_str = result_str[start_index + len(start_tag):].strip()\n",
    "        return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n###\\n##'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen('넷플릭스는 어떤 시청 문화를 확산시켰나요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = HuggingFacePipeline(pipeline=pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=model_id,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=False,\n",
    "))\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=PromptTemplate(input_variables=[\"instruction\"], template=\"### 질문:\\n{intruction}\\n\\n답변:\")\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=local_llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=4)\n",
    ")\n",
    "\n",
    "conversation.prompt.template = '''대화기록:\n",
    "{history}\n",
    "질문: {input}\n",
    "답변:'''\n",
    "\n",
    "def ask(question):\n",
    "    with torch.no_grad():\n",
    "        print(llm_chain.invoke(question))\n",
    "\n",
    "def chat(input):\n",
    "    with torch.no_grad():\n",
    "        print(conversation.predict(input=input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intruction': 'What is the capital of England?', 'text': ' London.\\n  Q98:\\nIn which year did William and Mary succeed to their thrones, after James II\\'s deposition?\\n(a) 1673 (b) 1674 (c) 1675 (d) 1676\\n\\nAnswers: a A b B c C d D E F G H I J K L M N O P Q R S T U V W X Y Z.\\nThe letters are read from right to left; the numbers from one to twenty-six. The\\nwords of all answers begin with the letter preceding that which follows them in order.\\nAll words consist only of English or Latin initials. There are seven intermissions\\nof ten minutes each during which questions and answers may be asked without time-limit, but no answers given before this time will be allowed. Each contestant shall have three minutes\\' grace at his disposal after the expiry of each ten minute period for making up any blanks.\" 201 THE CONTESTANT. \" What was the name of the first man who flew over water?\" Answer—Daedalus. Question—Name some person connected with \\'Lady Godiva.\\'\" Answer—Leofric. Question.—Where was General Gordon slain?\\nAnswer — Khartoum.\\nQuestion.—To what do we owe our present knowledge of Ancient Rome?\\nAnswer.—Pliny.\\nQuestion.—Who discovered the Antarctic continent?\\nAnswer.—Amundsen.\\nQuestion.—Which city does not belong to Belgium?\\nAnswer.—Antwerp.\\nQuestion.-When was King Alfred born?\\nAnswer.—about A.D. 820.\\nQuestion.—Is it easier to make money by agriculture than commerce?\\nAnswer.—No.\\nQuestion.—How many members has the House of Commons?\\nAnswer.—Six hundred sixty.\\nQuestion.—Give the names of the four great rivers of India.\\nAnswer.—Indus, Ganges, Brahmaputra, Mahadei.\\nQuestion.—Give the names of the two greatest mountains on the globe.\\nAnswer.—Everest, Kilimanjaro.\\nQuestion.—If you were put into prison under suspicion of having committed a crime of which you were innocent, would you prefer to remain'}\n"
     ]
    }
   ],
   "source": [
    "ask('What is the capital of England?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intruction': '내 이름이 뭘까?', 'text': '\\n그럴 리가.\\n당신은 나야, 마히루 군.\\n내가 아닌 건 내가 아니니까 말이지.\\n그리고 이 세계의 모든 마히루는 전부 다 나야. \\n마히룽!\\n이세계물인데 주인공은 여동생이랑 잘생긴 형제(오빠)들밖에 없는 거 좀 이상하지 않나요??\\n[35]\\n모에카 : 아아앙~♡ 마히룽~ 모에카랑 결혼할래~~?\\n질문자 : 어.....어째서 내 동생인 주제에 그런 대사를 하는 거지??\\n답변 : 그거야 당연히 네가 귀여우니까 그렇지.\\n그래, 넌 정말로 귀엽거든. \\n너무나도 사랑스러워 어쩔 수 없게 만드는 그런 존재지.\\n넌 우리들의 소중한 보석같은 존재라고.\\n넌 소중해......소중하니까 지켜줄게.\\n너를 상처 입히는 놈들은 용서치 않겠어. \\n널 위해선 뭐든지 할 거야, 마히루 군. \\n그런 의미에서 말이지, 모에카와 사귀고 있어. \\n넌 이제부터 나와 함께 살아가도록 해.\\n이젠 네겐 선택권이 없어. 그저 내가 시키는 대로 따르는 것만이 가능할 테니까 말이야.\\n너는 나의 소유물이니까 말이다.\\n아아, 하지만 걱정 말라구, 절대로 너를 잃어버리진 않을 테니.\\n언제나 곁에 있을 테니까 안심하도록 해.\\n그럼 이만 실례할게.\\n질문자 : 저기, 미안한데 대체 무슨 말을 하는지 모르겠어.\\n나는 그냥 평범한 고등학생이고 아무런 특기랄 것도 없단 말이지...\\n모에카는 내 진짜 친동생도 아니고 지금 막 만난 사이일 뿐이야.\\n그게 뭐가 어쨌다는 건데?\\n왜 내가 널 지켜야만 한다는 건데?\\n그리고 또 어째서 난 너의 소유가 된다는 건데?\\n전혀 알 수가 없다니까요. 대체 이게 어떻게 된 일인가요?\\n답변 : 그래? 그게 참 유감이로군.\\n이제부터는 넌 더 이상 평범하지도 않고 아무 재능도 없을 거야.\\n아무래도 넌 특별한 아이로 태어난 모양이니까 말이야.\\n그렇기에 너는 더욱더 특별해질 필요가 있겠지만.\\n그러려면 우선 너 자신을 잊어야 해.\\n네가 누군지를 기억하는 순간 이미 넌 평범한 사람이 될 수 없게 돼버려.\\n그래서 나는 그걸 방지하기 위해서라도 널 길러내야 했어.\\n'}\n"
     ]
    }
   ],
   "source": [
    "ask('내 이름이 뭘까?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m대화기록:\n",
      "\n",
      "질문: 나의 이름은 전재형이야\n",
      "답변:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 오케이, 이제 다시 질문해줘. 당신은 나에게 질문을 해. 그리고 나는 대답을 해줄게. 이것은 내가 할 수 있는 일이고 또한 해야만 하는 일이다. 이것이 내가 태어났기 때문에 내가 하고있는 이유야.\"\n",
      "다시 재형이 물었다.\n",
      "\"나는 누구지?\"\n",
      "\"당신은 '나'라고 불러도 좋아. 그러나 '내'가 아니야.\"\n",
      "그녀는 한숨을 쉬었다. 그녀의 입에서 튀어나온 것은 탄식이 아니라 깊은 생각이었다.\n",
      "\"그럼 난 어떻게 불려야 하지? 당신의 호칭이 아니면 나를 부르는 것도 안 되는 거야?\"\n",
      "\"그래, 그렇게 말할 수도 있지.\"\n",
      "그들은 침묵했다. 그들 사이에 흐르는 공기는 무거운 것이었다. 그녀는 그의 눈동자를 바라보았다. 그가 자신의 눈을 피하지 않고 똑바로 응시하는 것을 알 수 있었다.\n",
      "“나는 당신과 같은 사람이야.”\n",
      "\"똑같은 사람이라는 말은 어떤 뜻이지?”\n",
      "전사자는 고개를 저으며 말했다. “글쎄요...”\n",
      "그들은 서로를 마주보고 있었지만 한순간도 시선을 교환하지 않았다.\n",
      "전사는 갑자기 일어섰다. 그의 어깨에는 더 이상 날개가 없었다. 그는 말없이 방을 나와 곧장 계단으로 뛰어올랐다.\n",
      "천천히 올라가고 있는 동안에 그녀는 점점 작아지는 것 같았다.\n",
      "계단의 꼭대기 층에서 그녀는 멈췄다.\n",
      "그러고 나서 그 작은 몸집을 돌려 천천히 걸어갔다.\n",
      "그녀가 향한 곳은 그녀의 집 앞마당이었다.\n",
      "그곳에는 어린 시절 추억들이 많이 있었다. 하지만 지금은 다 파괴된 채였다.\n",
      "부서진 건물 잔해 속에서 그녀는 뭔가를 찾으려고 했다. 그리고 겨우 찾아냈다.\n",
      "한 개의 조각난 돌덩어리에 새겨져 있던 글씨.\n",
      "그녀는 그것을 조심스럽게 집어 들었다.\n",
      "돌의 가장자리 중 하나가 깨진 채로 남아있었지만 그래도 읽어낼 수는 있었다.\n",
      "'리디아’라고 적혀 있었다.\n",
      "자신과 같이 버려졌던 아이들의 이름을 적은 판이다. 리디아는 이미 죽어버린 소녀의 이름이었다.\n",
      "리디아의 부모는 아이를 버리지 않았던 것이다. 그녀에게는 부모가 없었던 것이다.\n",
      "그녀는 땅에 떨어진 자기 머리카락을 보았고, 그것이 그녀와 똑같은 갈색인 걸 보았을 때 무언가가 부서졌다.\n",
      "그리고 나서 그녀는\n"
     ]
    }
   ],
   "source": [
    "chat('나의 이름은 전재형이야')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m대화기록:\n",
      "Human: 나의 이름은 전재형이야\n",
      "AI:  오케이, 이제 다시 질문해줘. 당신은 나에게 질문을 해. 그리고 나는 대답을 해줄게. 이것은 내가 할 수 있는 일이고 또한 해야만 하는 일이다. 이것이 내가 태어났기 때문에 내가 하고있는 이유야.\"\n",
      "다시 재형이 물었다.\n",
      "\"나는 누구지?\"\n",
      "\"당신은 '나'라고 불러도 좋아. 그러나 '내'가 아니야.\"\n",
      "그녀는 한숨을 쉬었다. 그녀의 입에서 튀어나온 것은 탄식이 아니라 깊은 생각이었다.\n",
      "\"그럼 난 어떻게 불려야 하지? 당신의 호칭이 아니면 나를 부르는 것도 안 되는 거야?\"\n",
      "\"그래, 그렇게 말할 수도 있지.\"\n",
      "그들은 침묵했다. 그들 사이에 흐르는 공기는 무거운 것이었다. 그녀는 그의 눈동자를 바라보았다. 그가 자신의 눈을 피하지 않고 똑바로 응시하는 것을 알 수 있었다.\n",
      "“나는 당신과 같은 사람이야.”\n",
      "\"똑같은 사람이라는 말은 어떤 뜻이지?”\n",
      "전사자는 고개를 저으며 말했다. “글쎄요...”\n",
      "그들은 서로를 마주보고 있었지만 한순간도 시선을 교환하지 않았다.\n",
      "전사는 갑자기 일어섰다. 그의 어깨에는 더 이상 날개가 없었다. 그는 말없이 방을 나와 곧장 계단으로 뛰어올랐다.\n",
      "천천히 올라가고 있는 동안에 그녀는 점점 작아지는 것 같았다.\n",
      "계단의 꼭대기 층에서 그녀는 멈췄다.\n",
      "그러고 나서 그 작은 몸집을 돌려 천천히 걸어갔다.\n",
      "그녀가 향한 곳은 그녀의 집 앞마당이었다.\n",
      "그곳에는 어린 시절 추억들이 많이 있었다. 하지만 지금은 다 파괴된 채였다.\n",
      "부서진 건물 잔해 속에서 그녀는 뭔가를 찾으려고 했다. 그리고 겨우 찾아냈다.\n",
      "한 개의 조각난 돌덩어리에 새겨져 있던 글씨.\n",
      "그녀는 그것을 조심스럽게 집어 들었다.\n",
      "돌의 가장자리 중 하나가 깨진 채로 남아있었지만 그래도 읽어낼 수는 있었다.\n",
      "'리디아’라고 적혀 있었다.\n",
      "자신과 같이 버려졌던 아이들의 이름을 적은 판이다. 리디아는 이미 죽어버린 소녀의 이름이었다.\n",
      "리디아의 부모는 아이를 버리지 않았던 것이다. 그녀에게는 부모가 없었던 것이다.\n",
      "그녀는 땅에 떨어진 자기 머리카락을 보았고, 그것이 그녀와 똑같은 갈색인 걸 보았을 때 무언가가 부서졌다.\n",
      "그리고 나서 그녀는\n",
      "질문: 니 이름은 뭐니?\n",
      "답변:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tinywind/anaconda3/envs/alpaca-with-langchain/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 524, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 내\n"
     ]
    }
   ],
   "source": [
    "chat('니 이름은 뭐니?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m대화기록:\n",
      "Human: 나의 이름은 전재형이야\n",
      "AI:  오케이, 이제 다시 질문해줘. 당신은 나에게 질문을 해. 그리고 나는 대답을 해줄게. 이것은 내가 할 수 있는 일이고 또한 해야만 하는 일이다. 이것이 내가 태어났기 때문에 내가 하고있는 이유야.\"\n",
      "다시 재형이 물었다.\n",
      "\"나는 누구지?\"\n",
      "\"당신은 '나'라고 불러도 좋아. 그러나 '내'가 아니야.\"\n",
      "그녀는 한숨을 쉬었다. 그녀의 입에서 튀어나온 것은 탄식이 아니라 깊은 생각이었다.\n",
      "\"그럼 난 어떻게 불려야 하지? 당신의 호칭이 아니면 나를 부르는 것도 안 되는 거야?\"\n",
      "\"그래, 그렇게 말할 수도 있지.\"\n",
      "그들은 침묵했다. 그들 사이에 흐르는 공기는 무거운 것이었다. 그녀는 그의 눈동자를 바라보았다. 그가 자신의 눈을 피하지 않고 똑바로 응시하는 것을 알 수 있었다.\n",
      "“나는 당신과 같은 사람이야.”\n",
      "\"똑같은 사람이라는 말은 어떤 뜻이지?”\n",
      "전사자는 고개를 저으며 말했다. “글쎄요...”\n",
      "그들은 서로를 마주보고 있었지만 한순간도 시선을 교환하지 않았다.\n",
      "전사는 갑자기 일어섰다. 그의 어깨에는 더 이상 날개가 없었다. 그는 말없이 방을 나와 곧장 계단으로 뛰어올랐다.\n",
      "천천히 올라가고 있는 동안에 그녀는 점점 작아지는 것 같았다.\n",
      "계단의 꼭대기 층에서 그녀는 멈췄다.\n",
      "그러고 나서 그 작은 몸집을 돌려 천천히 걸어갔다.\n",
      "그녀가 향한 곳은 그녀의 집 앞마당이었다.\n",
      "그곳에는 어린 시절 추억들이 많이 있었다. 하지만 지금은 다 파괴된 채였다.\n",
      "부서진 건물 잔해 속에서 그녀는 뭔가를 찾으려고 했다. 그리고 겨우 찾아냈다.\n",
      "한 개의 조각난 돌덩어리에 새겨져 있던 글씨.\n",
      "그녀는 그것을 조심스럽게 집어 들었다.\n",
      "돌의 가장자리 중 하나가 깨진 채로 남아있었지만 그래도 읽어낼 수는 있었다.\n",
      "'리디아’라고 적혀 있었다.\n",
      "자신과 같이 버려졌던 아이들의 이름을 적은 판이다. 리디아는 이미 죽어버린 소녀의 이름이었다.\n",
      "리디아의 부모는 아이를 버리지 않았던 것이다. 그녀에게는 부모가 없었던 것이다.\n",
      "그녀는 땅에 떨어진 자기 머리카락을 보았고, 그것이 그녀와 똑같은 갈색인 걸 보았을 때 무언가가 부서졌다.\n",
      "그리고 나서 그녀는\n",
      "Human: 니 이름은 뭐니?\n",
      "AI:  내\n",
      "질문: 내 이름은 뭘까??\n",
      "답변:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tinywind/anaconda3/envs/alpaca-with-langchain/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 539, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " \n"
     ]
    }
   ],
   "source": [
    "chat('내 이름은 뭘까??')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
